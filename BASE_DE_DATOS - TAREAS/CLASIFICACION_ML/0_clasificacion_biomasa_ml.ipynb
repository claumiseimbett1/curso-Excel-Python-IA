{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de Biomasa usando Machine Learning\n",
    "## Clasificación de biomasa en categorías: Baja, Media, Alta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga y Análisis de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_excel('Base_Prediccion_Biomasa_Outliers1.xlsx', sheet_name='Datos Limpios')\n",
    "print(f\"Forma del dataset: {df.shape}\")\n",
    "\n",
    "# Análisis de la variable objetivo\n",
    "target_col = 'Categoria de Biomasa'\n",
    "print(f\"\\nDistribución de {target_col}:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nPorcentajes:\")\n",
    "print(df[target_col].value_counts(normalize=True) * 100)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de distribución de clases\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[target_col].value_counts().plot(kind='bar', color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Distribución de Categorías de Biomasa')\n",
    "plt.xlabel('Categoría')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df[target_col].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Proporción de Categorías de Biomasa')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para clasificación\n",
    "df_model = df.copy()\n",
    "\n",
    "# Eliminar variables no predictoras\n",
    "features_to_drop = ['Fecha de Medicion', 'ID_parcela', 'Biomasa_real Estadistica', 'Validacion']\n",
    "df_model = df_model.drop(columns=features_to_drop)\n",
    "\n",
    "# Definir variables\n",
    "y = df_model[target_col]\n",
    "X = df_model.drop(columns=[target_col])\n",
    "\n",
    "print(f\"Variables predictoras: {X.columns.tolist()}\")\n",
    "print(f\"Variable objetivo: {target_col}\")\n",
    "print(f\"Forma de X: {X.shape}, Forma de y: {y.shape}\")\n",
    "print(f\"\\nClases únicas: {y.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar variables categóricas en X\n",
    "le_features = LabelEncoder()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_features:\n",
    "    X[col] = le_features.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Codificar variable objetivo\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "print(f\"Codificación de clases:\")\n",
    "for i, clase in enumerate(le_target.classes_):\n",
    "    print(f\"{i}: {clase}\")\n",
    "\n",
    "print(f\"\\nVariables categóricas codificadas: {categorical_features.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de valores nulos\n",
    "print(\"Valores nulos antes de imputación:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Imputar valores nulos\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "print(f\"\\nDatos finales - X: {X_imputed.shape}, y: {len(y_encoded)}\")\n",
    "print(f\"Valores nulos restantes: {X_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. División de Datos y Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División 70/30 estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Datos de entrenamiento: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Datos de prueba: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Verificar distribución estratificada\n",
    "print(\"\\nDistribución en entrenamiento:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"{le_target.classes_[i]}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDistribución en prueba:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"{le_target.classes_[i]}: {count} ({count/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Datos escalados correctamente\")\n",
    "print(f\"Media de X_train_scaled: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std de X_train_scaled: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar modelos de clasificación\n",
    "def evaluate_classifier(name, model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Evalúa un clasificador y retorna métricas completas\"\"\"\n",
    "    # Entrenar\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Métricas básicas\n",
    "    metrics = {\n",
    "        'Accuracy_train': accuracy_score(y_train, y_pred_train),\n",
    "        'Accuracy_test': accuracy_score(y_test, y_pred_test),\n",
    "        'Precision_test': precision_score(y_test, y_pred_test, average='weighted'),\n",
    "        'Recall_test': recall_score(y_test, y_pred_test, average='weighted'),\n",
    "        'F1_test': f1_score(y_test, y_pred_test, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    metrics['CV_Accuracy_mean'] = cv_scores.mean()\n",
    "    metrics['CV_Accuracy_std'] = cv_scores.std()\n",
    "    \n",
    "    # AUC para multiclase (One-vs-Rest)\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            auc_score = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')\n",
    "            metrics['AUC_test'] = auc_score\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            y_scores = model.decision_function(X_test)\n",
    "            if y_scores.ndim == 1:  # Binario\n",
    "                auc_score = roc_auc_score(y_test, y_scores)\n",
    "            else:  # Multiclase\n",
    "                auc_score = roc_auc_score(y_test, y_scores, multi_class='ovr', average='weighted')\n",
    "            metrics['AUC_test'] = auc_score\n",
    "        else:\n",
    "            metrics['AUC_test'] = np.nan\n",
    "    except:\n",
    "        metrics['AUC_test'] = np.nan\n",
    "    \n",
    "    return metrics, y_pred_test\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "results_clf = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM con kernel linear\n",
    "svm_linear_params = {'C': [0.1, 1, 10, 100]}\n",
    "svm_linear = GridSearchCV(SVC(kernel='linear', random_state=42), svm_linear_params, cv=5, scoring='accuracy')\n",
    "svm_linear_metrics, svm_linear_pred = evaluate_classifier('SVM Linear', svm_linear, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results_clf['SVM Linear'] = svm_linear_metrics\n",
    "\n",
    "print(\"=== SVM LINEAR ===\")\n",
    "print(f\"Mejor C: {svm_linear.best_params_['C']}\")\n",
    "for metric, value in svm_linear_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM con kernel RBF\n",
    "svm_rbf_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]\n",
    "}\n",
    "svm_rbf = GridSearchCV(SVC(kernel='rbf', random_state=42), svm_rbf_params, cv=5, scoring='accuracy')\n",
    "svm_rbf_metrics, svm_rbf_pred = evaluate_classifier('SVM RBF', svm_rbf, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results_clf['SVM RBF'] = svm_rbf_metrics\n",
    "\n",
    "print(\"\\n=== SVM RBF ===\")\n",
    "print(f\"Mejores parámetros: {svm_rbf.best_params_}\")\n",
    "for metric, value in svm_rbf_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM con kernel polynomial\n",
    "svm_poly_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_poly = GridSearchCV(SVC(kernel='poly', random_state=42), svm_poly_params, cv=5, scoring='accuracy')\n",
    "svm_poly_metrics, svm_poly_pred = evaluate_classifier('SVM Polynomial', svm_poly, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results_clf['SVM Polynomial'] = svm_poly_metrics\n",
    "\n",
    "print(\"\\n=== SVM POLYNOMIAL ===\")\n",
    "print(f\"Mejores parámetros: {svm_poly.best_params_}\")\n",
    "for metric, value in svm_poly_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf_clf = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy')\n",
    "rf_metrics, rf_pred = evaluate_classifier('Random Forest', rf_clf, X_train, X_test, y_train, y_test)\n",
    "results_clf['Random Forest'] = rf_metrics\n",
    "\n",
    "print(\"=== RANDOM FOREST ===\")\n",
    "print(f\"Mejores parámetros: {rf_clf.best_params_}\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Importancia de características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_imputed.columns,\n",
    "    'importance': rf_clf.best_estimator_.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nImportancia de características:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "gb_clf = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=5, scoring='accuracy')\n",
    "gb_metrics, gb_pred = evaluate_classifier('Gradient Boosting', gb_clf, X_train, X_test, y_train, y_test)\n",
    "results_clf['Gradient Boosting'] = gb_metrics\n",
    "\n",
    "print(\"=== GRADIENT BOOSTING ===\")\n",
    "print(f\"Mejores parámetros: {gb_clf.best_params_}\")\n",
    "for metric, value in gb_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_params = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "lr_clf = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000), lr_params, cv=5, scoring='accuracy')\n",
    "lr_metrics, lr_pred = evaluate_classifier('Logistic Regression', lr_clf, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results_clf['Logistic Regression'] = lr_metrics\n",
    "\n",
    "print(\"=== LOGISTIC REGRESSION ===\")\n",
    "print(f\"Mejores parámetros: {lr_clf.best_params_}\")\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_params = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "dt_clf = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_params, cv=5, scoring='accuracy')\n",
    "dt_metrics, dt_pred = evaluate_classifier('Decision Tree', dt_clf, X_train, X_test, y_train, y_test)\n",
    "results_clf['Decision Tree'] = dt_metrics\n",
    "\n",
    "print(\"=== DECISION TREE ===\")\n",
    "print(f\"Mejores parámetros: {dt_clf.best_params_}\")\n",
    "for metric, value in dt_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn_clf = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, scoring='accuracy')\n",
    "knn_metrics, knn_pred = evaluate_classifier('KNN', knn_clf, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results_clf['KNN'] = knn_metrics\n",
    "\n",
    "print(\"=== K-NEAREST NEIGHBORS ===\")\n",
    "print(f\"Mejores parámetros: {knn_clf.best_params_}\")\n",
    "for metric, value in knn_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb_clf = GaussianNB()\n",
    "nb_metrics, nb_pred = evaluate_classifier('Naive Bayes', nb_clf, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "results_clf['Naive Bayes'] = nb_metrics\n",
    "\n",
    "print(\"=== NAIVE BAYES ===\")\n",
    "for metric, value in nb_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con resultados\n",
    "results_df_clf = pd.DataFrame(results_clf).T\n",
    "results_df_clf = results_df_clf.round(4)\n",
    "\n",
    "print(\"=== COMPARACIÓN DE MODELOS DE CLASIFICACIÓN ===\")\n",
    "print(results_df_clf)\n",
    "\n",
    "# Ordenar por accuracy de test\n",
    "results_sorted_clf = results_df_clf.sort_values('Accuracy_test', ascending=False)\n",
    "print(\"\\n=== RANKING POR ACCURACY DE TEST ===\")\n",
    "print(results_sorted_clf[['Accuracy_test', 'Precision_test', 'Recall_test', 'F1_test', 'CV_Accuracy_mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de comparación\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "models = list(results_clf.keys())\n",
    "accuracy_train = [results_clf[model]['Accuracy_train'] for model in models]\n",
    "accuracy_test = [results_clf[model]['Accuracy_test'] for model in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "# Accuracy\n",
    "axes[0,0].bar(x - width/2, accuracy_train, width, label='Train', alpha=0.8)\n",
    "axes[0,0].bar(x + width/2, accuracy_test, width, label='Test', alpha=0.8)\n",
    "axes[0,0].set_xlabel('Modelos')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].set_title('Comparación Accuracy')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(models, rotation=45)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision_test = [results_clf[model]['Precision_test'] for model in models]\n",
    "recall_test = [results_clf[model]['Recall_test'] for model in models]\n",
    "f1_test = [results_clf[model]['F1_test'] for model in models]\n",
    "\n",
    "width = 0.25\n",
    "axes[0,1].bar(x - width, precision_test, width, label='Precision', alpha=0.8)\n",
    "axes[0,1].bar(x, recall_test, width, label='Recall', alpha=0.8)\n",
    "axes[0,1].bar(x + width, f1_test, width, label='F1-Score', alpha=0.8)\n",
    "axes[0,1].set_xlabel('Modelos')\n",
    "axes[0,1].set_ylabel('Score')\n",
    "axes[0,1].set_title('Precision, Recall y F1-Score')\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(models, rotation=45)\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation Accuracy\n",
    "cv_accuracy_mean = [results_clf[model]['CV_Accuracy_mean'] for model in models]\n",
    "cv_accuracy_std = [results_clf[model]['CV_Accuracy_std'] for model in models]\n",
    "\n",
    "axes[1,0].bar(x, cv_accuracy_mean, yerr=cv_accuracy_std, capsize=5, alpha=0.8)\n",
    "axes[1,0].set_xlabel('Modelos')\n",
    "axes[1,0].set_ylabel('CV Accuracy')\n",
    "axes[1,0].set_title('Cross-Validation Accuracy')\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels(models, rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC Score\n",
    "auc_test = [results_clf[model]['AUC_test'] if not np.isnan(results_clf[model]['AUC_test']) else 0 for model in models]\n",
    "axes[1,1].bar(x, auc_test, alpha=0.8)\n",
    "axes[1,1].set_xlabel('Modelos')\n",
    "axes[1,1].set_ylabel('AUC Score')\n",
    "axes[1,1].set_title('AUC Score (One-vs-Rest)')\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(models, rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis Detallado del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar el mejor modelo\n",
    "best_model_name = results_sorted_clf.index[0]\n",
    "print(f\"Mejor modelo: {best_model_name}\")\n",
    "print(f\"Accuracy de test: {results_sorted_clf.loc[best_model_name, 'Accuracy_test']:.4f}\")\n",
    "print(f\"F1-Score de test: {results_sorted_clf.loc[best_model_name, 'F1_test']:.4f}\")\n",
    "\n",
    "# Obtener predicciones del mejor modelo\n",
    "if best_model_name == 'SVM Linear':\n",
    "    best_predictions = svm_linear_pred\n",
    "    best_model = svm_linear\n",
    "elif best_model_name == 'SVM RBF':\n",
    "    best_predictions = svm_rbf_pred\n",
    "    best_model = svm_rbf\n",
    "elif best_model_name == 'SVM Polynomial':\n",
    "    best_predictions = svm_poly_pred\n",
    "    best_model = svm_poly\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_predictions = rf_pred\n",
    "    best_model = rf_clf\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_predictions = gb_pred\n",
    "    best_model = gb_clf\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    best_predictions = lr_pred\n",
    "    best_model = lr_clf\n",
    "elif best_model_name == 'Decision Tree':\n",
    "    best_predictions = dt_pred\n",
    "    best_model = dt_clf\n",
    "elif best_model_name == 'KNN':\n",
    "    best_predictions = knn_pred\n",
    "    best_model = knn_clf\n",
    "else:  # Naive Bayes\n",
    "    best_predictions = nb_pred\n",
    "    best_model = nb_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Matriz de Confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_target.classes_, \n",
    "            yticklabels=le_target.classes_)\n",
    "plt.title(f'Matriz de Confusión - {best_model_name}')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.show()\n",
    "\n",
    "# Calcular métricas por clase\n",
    "print(f\"\\n=== MÉTRICAS DETALLADAS - {best_model_name} ===\")\n",
    "print(classification_report(y_test, best_predictions, target_names=le_target.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Métricas de Error Detalladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de error por clase\n",
    "precision_per_class = precision_score(y_test, best_predictions, average=None)\n",
    "recall_per_class = recall_score(y_test, best_predictions, average=None)\n",
    "f1_per_class = f1_score(y_test, best_predictions, average=None)\n",
    "\n",
    "metrics_per_class = pd.DataFrame({\n",
    "    'Clase': le_target.classes_,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class\n",
    "})\n",
    "\n",
    "print(\"=== MÉTRICAS POR CLASE ===\")\n",
    "print(metrics_per_class.round(4))\n",
    "\n",
    "# Errores de clasificación\n",
    "errors = (y_test != best_predictions)\n",
    "error_rate = errors.sum() / len(y_test)\n",
    "print(f\"\\nTasa de error: {error_rate:.4f} ({error_rate*100:.2f}%)\")\n",
    "print(f\"Errores totales: {errors.sum()} de {len(y_test)} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de métricas por clase\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "classes = le_target.classes_\n",
    "x_pos = np.arange(len(classes))\n",
    "\n",
    "# Precision\n",
    "axes[0].bar(x_pos, precision_per_class, alpha=0.8, color='skyblue')\n",
    "axes[0].set_xlabel('Clases')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('Precision por Clase')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(classes)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1].bar(x_pos, recall_per_class, alpha=0.8, color='lightgreen')\n",
    "axes[1].set_xlabel('Clases')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title('Recall por Clase')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "axes[2].bar(x_pos, f1_per_class, alpha=0.8, color='salmon')\n",
    "axes[2].set_xlabel('Clases')\n",
    "axes[2].set_ylabel('F1-Score')\n",
    "axes[2].set_title('F1-Score por Clase')\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels(classes)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis de Características (si aplica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si el mejor modelo es Random Forest o tiene feature importances\n",
    "if best_model_name == 'Random Forest':\n",
    "    print(\"=== IMPORTANCIA DE CARACTERÍSTICAS - RANDOM FOREST ===\")\n",
    "    \n",
    "    # Gráfico de importancia\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance_sorted = feature_importance.sort_values('importance', ascending=True)\n",
    "    plt.barh(range(len(feature_importance_sorted)), feature_importance_sorted['importance'])\n",
    "    plt.yticks(range(len(feature_importance_sorted)), feature_importance_sorted['feature'])\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title('Importancia de Características - Random Forest')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(feature_importance)\n",
    "\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    print(\"=== IMPORTANCIA DE CARACTERÍSTICAS - GRADIENT BOOSTING ===\")\n",
    "    gb_importance = pd.DataFrame({\n",
    "        'feature': X_imputed.columns,\n",
    "        'importance': gb_clf.best_estimator_.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    gb_importance_sorted = gb_importance.sort_values('importance', ascending=True)\n",
    "    plt.barh(range(len(gb_importance_sorted)), gb_importance_sorted['importance'])\n",
    "    plt.yticks(range(len(gb_importance_sorted)), gb_importance_sorted['feature'])\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title('Importancia de Características - Gradient Boosting')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(gb_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumen y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RESUMEN FINAL - CLASIFICACIÓN DE BIOMASA ===\")\n",
    "print(f\"Dataset: {df.shape[0]} filas, {len(X_imputed.columns)} variables predictoras\")\n",
    "print(f\"Clases: {', '.join(le_target.classes_)}\")\n",
    "print(f\"División: {len(y_train)} entrenamiento, {len(y_test)} prueba\")\n",
    "\n",
    "print(f\"\\nMejor modelo: {best_model_name}\")\n",
    "print(f\"Accuracy: {results_sorted_clf.loc[best_model_name, 'Accuracy_test']:.4f}\")\n",
    "print(f\"Precision: {results_sorted_clf.loc[best_model_name, 'Precision_test']:.4f}\")\n",
    "print(f\"Recall: {results_sorted_clf.loc[best_model_name, 'Recall_test']:.4f}\")\n",
    "print(f\"F1-Score: {results_sorted_clf.loc[best_model_name, 'F1_test']:.4f}\")\n",
    "\n",
    "print(\"\\n=== TOP 3 MODELOS ===\")\n",
    "for i, (model_name, metrics) in enumerate(results_sorted_clf.head(3).iterrows()):\n",
    "    print(f\"{i+1}. {model_name}: Accuracy={metrics['Accuracy_test']:.4f}, F1={metrics['F1_test']:.4f}\")\n",
    "\n",
    "print(\"\\n=== DISTRIBUCIÓN DE ERRORES POR CLASE ===\")\n",
    "for i, clase in enumerate(le_target.classes_):\n",
    "    class_errors = ((y_test == i) & (best_predictions != i)).sum()\n",
    "    class_total = (y_test == i).sum()\n",
    "    if class_total > 0:\n",
    "        error_rate_class = class_errors / class_total\n",
    "        print(f\"{clase}: {class_errors}/{class_total} errores ({error_rate_class*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nError total: {errors.sum()}/{len(y_test)} ({error_rate*100:.2f}%)\")\n",
    "print(f\"Exactitud total: {(1-error_rate)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Generación Automatizada de Reportes en Excel",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side",
    "from openpyxl.utils.dataframe import dataframe_to_rows",
    "from datetime import datetime",
    "import pandas as pd",
    "import numpy as np",
    "from sklearn.metrics import accuracy_score",
    "",
    "# Crear reporte Excel automatizado para clasificación",
    "def generar_reporte_excel_clasificacion(best_model_name=None, best_predictions=None, results_sorted=None,",
    "                         y_test=None, X_train=None, X_test=None, df=None, X_imputed=None, le_target=None):",
    "    \"\"\"Genera un reporte completo en Excel con todos los resultados del análisis de clasificación\"\"\"",
    "",
    "    # Nombre del archivo",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
    "    filename = f'Reporte_Clasificacion_Biomasa_{timestamp}.xlsx'",
    "",
    "    # Crear workbook",
    "    wb = Workbook()",
    "",
    "    # Estilos",
    "    header_fill = PatternFill(start_color=\"4CAF50\", end_color=\"4CAF50\", fill_type=\"solid\")",
    "    header_font = Font(bold=True, color=\"FFFFFF\", size=12)",
    "    title_font = Font(bold=True, size=14, color=\"1F4788\")",
    "    border = Border(",
    "        left=Side(style='thin'),",
    "        right=Side(style='thin'),",
    "        top=Side(style='thin'),",
    "        bottom=Side(style='thin')",
    "    )",
    "",
    "    # ============ HOJA 1: RESUMEN EJECUTIVO ============",
    "    ws1 = wb.active",
    "    ws1.title = \"Resumen Ejecutivo\"",
    "",
    "    # Título",
    "    ws1['A1'] = 'REPORTE DE CLASIFICACIÓN DE BIOMASA - MACHINE LEARNING'",
    "    ws1['A1'].font = Font(bold=True, size=16, color=\"1F4788\")",
    "    ws1.merge_cells('A1:D1')",
    "",
    "    ws1['A2'] = f'Fecha de generación: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'",
    "    ws1['A2'].font = Font(italic=True)",
    "    ws1.merge_cells('A2:D2')",
    "",
    "    # Información del dataset",
    "    row = 4",
    "    ws1[f'A{row}'] = 'INFORMACIÓN DEL DATASET'",
    "    ws1[f'A{row}'].font = title_font",
    "    row += 1",
    "",
    "    dataset_info = [",
    "        ['Total de registros:', df.shape[0]],",
    "        ['Registros válidos para modelado:', X_imputed.shape[0]],",
    "        ['Variables predictoras:', X_imputed.shape[1]],",
    "        ['Clases objetivo:', ', '.join(le_target.classes_)],",
    "        ['División entrenamiento:', f'{X_train.shape[0]} ({X_train.shape[0]/X_imputed.shape[0]*100:.1f}%)'],",
    "        ['División prueba:', f'{X_test.shape[0]} ({X_test.shape[0]/X_imputed.shape[0]*100:.1f}%)'],",
    "    ]",
    "",
    "    for item in dataset_info:",
    "        ws1[f'A{row}'] = item[0]",
    "        ws1[f'B{row}'] = item[1]",
    "        ws1[f'A{row}'].font = Font(bold=True)",
    "        row += 1",
    "",
    "    # Mejor modelo",
    "    row += 2",
    "    ws1[f'A{row}'] = 'MEJOR MODELO SELECCIONADO'",
    "    ws1[f'A{row}'].font = title_font",
    "    row += 1",
    "",
    "    ws1[f'A{row}'] = 'Modelo:'",
    "    ws1[f'B{row}'] = best_model_name",
    "    ws1[f'A{row}'].font = Font(bold=True)",
    "    ws1[f'B{row}'].font = Font(bold=True, size=12, color=\"006400\")",
    "    row += 1",
    "",
    "    best_model_info = [",
    "        ['Accuracy (Test):', f\"{results_sorted.loc[best_model_name, 'Accuracy_test']:.4f}\"],",
    "        ['Precision (Test):', f\"{results_sorted.loc[best_model_name, 'Precision_test']:.4f}\"],",
    "        ['Recall (Test):', f\"{results_sorted.loc[best_model_name, 'Recall_test']:.4f}\"],",
    "        ['F1-Score (Test):', f\"{results_sorted.loc[best_model_name, 'F1_test']:.4f}\"],",
    "        ['CV Accuracy Media:', f\"{results_sorted.loc[best_model_name, 'CV_Accuracy_mean']:.4f}\"],",
    "    ]",
    "",
    "    for item in best_model_info:",
    "        ws1[f'A{row}'] = item[0]",
    "        ws1[f'B{row}'] = item[1]",
    "        ws1[f'A{row}'].font = Font(bold=True)",
    "        row += 1",
    "",
    "    # Top 3 modelos",
    "    row += 2",
    "    ws1[f'A{row}'] = 'TOP 3 MODELOS'",
    "    ws1[f'A{row}'].font = title_font",
    "    row += 1",
    "",
    "    # Encabezados",
    "    headers = ['Ranking', 'Modelo', 'Accuracy', 'Precision', 'Recall', 'F1-Score']",
    "    for col, header in enumerate(headers, start=1):",
    "        cell = ws1.cell(row=row, column=col)",
    "        cell.value = header",
    "        cell.font = header_font",
    "        cell.fill = header_fill",
    "        cell.alignment = Alignment(horizontal='center')",
    "        cell.border = border",
    "    row += 1",
    "",
    "    # Datos top 3",
    "    for i, (model_name, metrics) in enumerate(results_sorted.head(3).iterrows(), start=1):",
    "        ws1[f'A{row}'] = i",
    "        ws1[f'B{row}'] = model_name",
    "        ws1[f'C{row}'] = f\"{metrics['Accuracy_test']:.4f}\"",
    "        ws1[f'D{row}'] = f\"{metrics['Precision_test']:.4f}\"",
    "        ws1[f'E{row}'] = f\"{metrics['Recall_test']:.4f}\"",
    "        ws1[f'F{row}'] = f\"{metrics['F1_test']:.4f}\"",
    "",
    "        for col in range(1, 7):",
    "            ws1.cell(row=row, column=col).border = border",
    "            ws1.cell(row=row, column=col).alignment = Alignment(horizontal='center')",
    "        row += 1",
    "",
    "    # Ajustar anchos",
    "    ws1.column_dimensions['A'].width = 25",
    "    ws1.column_dimensions['B'].width = 25",
    "    ws1.column_dimensions['C'].width = 15",
    "    ws1.column_dimensions['D'].width = 15",
    "    ws1.column_dimensions['E'].width = 15",
    "    ws1.column_dimensions['F'].width = 15",
    "",
    "    # ============ HOJA 2: COMPARACIÓN DE MODELOS ============",
    "    ws2 = wb.create_sheet(\"Comparación Modelos\")",
    "",
    "    ws2['A1'] = 'COMPARACIÓN DETALLADA DE MODELOS DE CLASIFICACIÓN'",
    "    ws2['A1'].font = title_font",
    "    ws2.merge_cells('A1:I1')",
    "",
    "    # Encabezados",
    "    headers = ['Modelo', 'Acc Train', 'Acc Test', 'Precision', 'Recall', ",
    "               'F1-Score', 'CV Acc Mean', 'CV Acc Std', 'AUC']",
    "    for col, header in enumerate(headers, start=1):",
    "        cell = ws2.cell(row=3, column=col)",
    "        cell.value = header",
    "        cell.font = header_font",
    "        cell.fill = header_fill",
    "        cell.alignment = Alignment(horizontal='center')",
    "        cell.border = border",
    "",
    "    # Datos",
    "    row = 4",
    "    for model_name, metrics in results_sorted.iterrows():",
    "        ws2[f'A{row}'] = model_name",
    "        ws2[f'B{row}'] = f\"{metrics['Accuracy_train']:.4f}\"",
    "        ws2[f'C{row}'] = f\"{metrics['Accuracy_test']:.4f}\"",
    "        ws2[f'D{row}'] = f\"{metrics['Precision_test']:.4f}\"",
    "        ws2[f'E{row}'] = f\"{metrics['Recall_test']:.4f}\"",
    "        ws2[f'F{row}'] = f\"{metrics['F1_test']:.4f}\"",
    "        ws2[f'G{row}'] = f\"{metrics['CV_Accuracy_mean']:.4f}\"",
    "        ws2[f'H{row}'] = f\"{metrics['CV_Accuracy_std']:.4f}\"",
    "        if not np.isnan(metrics['AUC_test']):",
    "            ws2[f'I{row}'] = f\"{metrics['AUC_test']:.4f}\"",
    "        else:",
    "            ws2[f'I{row}'] = 'N/A'",
    "",
    "        # Resaltar mejor modelo",
    "        if model_name == best_model_name:",
    "            for col in range(1, 10):",
    "                ws2.cell(row=row, column=col).fill = PatternFill(",
    "                    start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\"",
    "                )",
    "",
    "        for col in range(1, 10):",
    "            ws2.cell(row=row, column=col).border = border",
    "            ws2.cell(row=row, column=col).alignment = Alignment(horizontal='center')",
    "        row += 1",
    "",
    "    # Ajustar anchos",
    "    for col in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']:",
    "        ws2.column_dimensions[col].width = 15",
    "",
    "    # ============ HOJA 3: PREDICCIONES Y MATRIZ DE CONFUSIÓN ============",
    "    ws3 = wb.create_sheet(\"Predicciones\")",
    "",
    "    ws3['A1'] = 'PREDICCIONES DEL MEJOR MODELO'",
    "    ws3['A1'].font = title_font",
    "    ws3.merge_cells('A1:D1')",
    "",
    "    ws3['A2'] = f'Modelo: {best_model_name}'",
    "    ws3['A2'].font = Font(bold=True)",
    "    ws3.merge_cells('A2:D2')",
    "",
    "    # Crear DataFrame con predicciones",
    "    predicciones_df = pd.DataFrame({",
    "        'ID': range(1, len(y_test) + 1),",
    "        'Clase Real': [le_target.classes_[i] for i in y_test],",
    "        'Clase Predicha': [le_target.classes_[i] for i in best_predictions],",
    "        'Correcto': ['Sí' if y_test[i] == best_predictions[i] else 'No' for i in range(len(y_test))]",
    "    })",
    "",
    "    # Encabezados",
    "    row = 4",
    "    for col, header in enumerate(predicciones_df.columns, start=1):",
    "        cell = ws3.cell(row=row, column=col)",
    "        cell.value = header",
    "        cell.font = header_font",
    "        cell.fill = header_fill",
    "        cell.alignment = Alignment(horizontal='center')",
    "        cell.border = border",
    "",
    "    # Datos",
    "    for r_idx, row_data in enumerate(dataframe_to_rows(predicciones_df, index=False, header=False), start=5):",
    "        for c_idx, value in enumerate(row_data, start=1):",
    "            cell = ws3.cell(row=r_idx, column=c_idx)",
    "            cell.value = value",
    "            cell.border = border",
    "            cell.alignment = Alignment(horizontal='center')",
    "",
    "            # Colorear según si es correcto",
    "            if c_idx == 4:  # Columna Correcto",
    "                if value == 'Sí':",
    "                    cell.fill = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")",
    "                else:",
    "                    cell.fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")",
    "",
    "    # Estadísticas",
    "    stats_row = len(predicciones_df) + 7",
    "    ws3[f'A{stats_row}'] = 'ESTADÍSTICAS DE CLASIFICACIÓN'",
    "    ws3[f'A{stats_row}'].font = title_font",
    "",
    "    stats_row += 1",
    "    total_correcto = (predicciones_df['Correcto'] == 'Sí').sum()",
    "    total = len(predicciones_df)",
    "    accuracy = total_correcto / total",
    "",
    "    error_stats = [",
    "        ['Total de muestras:', total],",
    "        ['Predicciones correctas:', total_correcto],",
    "        ['Predicciones incorrectas:', total - total_correcto],",
    "        ['Accuracy:', f\"{accuracy:.4f} ({accuracy*100:.2f}%)\"],",
    "        ['Precision:', f\"{results_sorted.loc[best_model_name, 'Precision_test']:.4f}\"],",
    "        ['Recall:', f\"{results_sorted.loc[best_model_name, 'Recall_test']:.4f}\"],",
    "        ['F1-Score:', f\"{results_sorted.loc[best_model_name, 'F1_test']:.4f}\"],",
    "    ]",
    "",
    "    for item in error_stats:",
    "        ws3[f'A{stats_row}'] = item[0]",
    "        ws3[f'B{stats_row}'] = item[1]",
    "        ws3[f'A{stats_row}'].font = Font(bold=True)",
    "        stats_row += 1",
    "",
    "    # Ajustar anchos",
    "    for col in ['A', 'B', 'C', 'D']:",
    "        ws3.column_dimensions[col].width = 20",
    "",
    "    # ============ HOJA 4: VARIABLES PREDICTORAS ============",
    "    ws4 = wb.create_sheet(\"Variables Predictoras\")",
    "",
    "    ws4['A1'] = 'VARIABLES PREDICTORAS UTILIZADAS'",
    "    ws4['A1'].font = title_font",
    "    ws4.merge_cells('A1:B1')",
    "",
    "    # Variables",
    "    row = 3",
    "    ws4[f'A{row}'] = 'Variable'",
    "    ws4[f'B{row}'] = 'Descripción'",
    "    for col in ['A', 'B']:",
    "        ws4[f'{col}{row}'].font = header_font",
    "        ws4[f'{col}{row}'].fill = header_fill",
    "        ws4[f'{col}{row}'].alignment = Alignment(horizontal='center')",
    "        ws4[f'{col}{row}'].border = border",
    "",
    "    row += 1",
    "    for var in X_imputed.columns:",
    "        ws4[f'A{row}'] = var",
    "        ws4[f'B{row}'] = 'Variable predictora'",
    "        for col in ['A', 'B']:",
    "            ws4[f'{col}{row}'].border = border",
    "        row += 1",
    "",
    "    # Ajustar anchos",
    "    ws4.column_dimensions['A'].width = 30",
    "    ws4.column_dimensions['B'].width = 50",
    "",
    "    # Guardar archivo",
    "    wb.save(filename)",
    "",
    "    print(f\"✓ Reporte generado exitosamente: {filename}\")",
    "    print(f\"  - Hoja 1: Resumen Ejecutivo\")",
    "    print(f\"  - Hoja 2: Comparación de Modelos\")",
    "    print(f\"  - Hoja 3: Predicciones\")",
    "    print(f\"  - Hoja 4: Variables Predictoras\")",
    "",
    "    return filename",
    "",
    "",
    "# Generar reporte (solo si las variables están definidas)",
    "if 'best_model_name' in globals() and 'results_sorted_clf' in globals():",
    "    archivo_reporte = generar_reporte_excel_clasificacion(",
    "        best_model_name, best_predictions, results_sorted_clf, ",
    "        y_test, X_train, X_test, df, X_imputed, le_target",
    "    )",
    "else:",
    "    print('⚠ Ejecute primero las celdas anteriores que entrenan y evalúan los modelos')",
    "    print('  Luego ejecute esta celda nuevamente para generar el reporte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dashboard de Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard completo de resultados de clasificación",
    "from datetime import datetime",
    "",
    "# Crear figura con subplots",
    "fig = plt.figure(figsize=(20, 12))",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)",
    "",
    "# 1. Métricas del mejor modelo (superior izquierda)",
    "ax1 = fig.add_subplot(gs[0, 0])",
    "metricas_best = [",
    "    results_clf[best_model_name]['Accuracy_test'],",
    "    results_clf[best_model_name]['Precision_test'],",
    "    results_clf[best_model_name]['Recall_test'],",
    "    results_clf[best_model_name]['F1_test']",
    "]",
    "labels_metricas = ['Accuracy', 'Precision', 'Recall', 'F1-Score']",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']",
    "bars = ax1.barh(labels_metricas, metricas_best, color=colors, alpha=0.8)",
    "ax1.set_xlim(0, 1)",
    "ax1.set_xlabel('Score', fontweight='bold')",
    "ax1.set_title(f'Métricas - {best_model_name}', fontsize=12, fontweight='bold', pad=20)",
    "ax1.grid(axis='x', alpha=0.3)",
    "for bar, value in zip(bars, metricas_best):",
    "    ax1.text(value + 0.02, bar.get_y() + bar.get_height()/2, f'{value:.4f}', ",
    "             va='center', fontweight='bold', fontsize=10)",
    "",
    "# 2. Matriz de Confusión (superior centro)",
    "from sklearn.metrics import confusion_matrix",
    "ax2 = fig.add_subplot(gs[0, 1])",
    "cm = confusion_matrix(y_test, best_predictions)",
    "im = ax2.imshow(cm, interpolation='nearest', cmap='Blues')",
    "ax2.figure.colorbar(im, ax=ax2)",
    "tick_marks = np.arange(len(le_target.classes_))",
    "ax2.set_xticks(tick_marks)",
    "ax2.set_yticks(tick_marks)",
    "ax2.set_xticklabels(le_target.classes_, rotation=45)",
    "ax2.set_yticklabels(le_target.classes_)",
    "# Agregar valores en las celdas",
    "fmt = 'd'",
    "thresh = cm.max() / 2.",
    "for i in range(cm.shape[0]):",
    "    for j in range(cm.shape[1]):",
    "        ax2.text(j, i, format(cm[i, j], fmt),",
    "                ha=\"center\", va=\"center\",",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",",
    "                fontweight='bold')",
    "ax2.set_ylabel('Clase Real', fontweight='bold')",
    "ax2.set_xlabel('Clase Predicha', fontweight='bold')",
    "ax2.set_title('Matriz de Confusión', fontsize=12, fontweight='bold', pad=20)",
    "",
    "# 3. Comparación de Accuracy (superior derecha)",
    "ax3 = fig.add_subplot(gs[0, 2])",
    "models = list(results_clf.keys())",
    "accuracy_train = [results_clf[model]['Accuracy_train'] for model in models]",
    "accuracy_test = [results_clf[model]['Accuracy_test'] for model in models]",
    "x = np.arange(len(models))",
    "width = 0.35",
    "ax3.bar(x - width/2, accuracy_train, width, label='Train', alpha=0.8, color='skyblue')",
    "ax3.bar(x + width/2, accuracy_test, width, label='Test', alpha=0.8, color='lightcoral')",
    "ax3.set_xlabel('Modelos', fontweight='bold')",
    "ax3.set_ylabel('Accuracy', fontweight='bold')",
    "ax3.set_title('Comparación de Accuracy', fontsize=12, fontweight='bold', pad=20)",
    "ax3.set_xticks(x)",
    "ax3.set_xticklabels(models, rotation=45, ha='right', fontsize=8)",
    "ax3.legend()",
    "ax3.grid(True, alpha=0.3, axis='y')",
    "",
    "# 4. Precision, Recall y F1-Score por modelo (media izquierda)",
    "ax4 = fig.add_subplot(gs[1, 0])",
    "precision_test = [results_clf[model]['Precision_test'] for model in models]",
    "recall_test = [results_clf[model]['Recall_test'] for model in models]",
    "f1_test = [results_clf[model]['F1_test'] for model in models]",
    "x = np.arange(len(models))",
    "width = 0.25",
    "ax4.bar(x - width, precision_test, width, label='Precision', alpha=0.8, color='#3498db')",
    "ax4.bar(x, recall_test, width, label='Recall', alpha=0.8, color='#e74c3c')",
    "ax4.bar(x + width, f1_test, width, label='F1-Score', alpha=0.8, color='#f39c12')",
    "ax4.set_xlabel('Modelos', fontweight='bold')",
    "ax4.set_ylabel('Score', fontweight='bold')",
    "ax4.set_title('Precision, Recall y F1-Score', fontsize=12, fontweight='bold', pad=20)",
    "ax4.set_xticks(x)",
    "ax4.set_xticklabels(models, rotation=45, ha='right', fontsize=8)",
    "ax4.legend()",
    "ax4.grid(True, alpha=0.3, axis='y')",
    "",
    "# 5. Cross-Validation Accuracy (media centro)",
    "ax5 = fig.add_subplot(gs[1, 1])",
    "cv_accuracy_mean = [results_clf[model]['CV_Accuracy_mean'] for model in models]",
    "cv_accuracy_std = [results_clf[model]['CV_Accuracy_std'] for model in models]",
    "ax5.bar(x, cv_accuracy_mean, yerr=cv_accuracy_std, capsize=5, alpha=0.8, color='#9b59b6')",
    "ax5.set_xlabel('Modelos', fontweight='bold')",
    "ax5.set_ylabel('CV Accuracy', fontweight='bold')",
    "ax5.set_title('Cross-Validation Accuracy (5-Fold)', fontsize=12, fontweight='bold', pad=20)",
    "ax5.set_xticks(x)",
    "ax5.set_xticklabels(models, rotation=45, ha='right', fontsize=8)",
    "ax5.grid(True, alpha=0.3, axis='y')",
    "",
    "# 6. Métricas por clase (media derecha)",
    "ax6 = fig.add_subplot(gs[1, 2])",
    "from sklearn.metrics import precision_score, recall_score, f1_score",
    "precision_per_class = precision_score(y_test, best_predictions, average=None)",
    "recall_per_class = recall_score(y_test, best_predictions, average=None)",
    "f1_per_class = f1_score(y_test, best_predictions, average=None)",
    "classes = le_target.classes_",
    "x_pos = np.arange(len(classes))",
    "width = 0.25",
    "ax6.bar(x_pos - width, precision_per_class, width, label='Precision', alpha=0.8, color='#3498db')",
    "ax6.bar(x_pos, recall_per_class, width, label='Recall', alpha=0.8, color='#e74c3c')",
    "ax6.bar(x_pos + width, f1_per_class, width, label='F1-Score', alpha=0.8, color='#f39c12')",
    "ax6.set_xlabel('Clases', fontweight='bold')",
    "ax6.set_ylabel('Score', fontweight='bold')",
    "ax6.set_title('Métricas por Clase', fontsize=12, fontweight='bold', pad=20)",
    "ax6.set_xticks(x_pos)",
    "ax6.set_xticklabels(classes, fontsize=10)",
    "ax6.legend()",
    "ax6.grid(True, alpha=0.3, axis='y')",
    "",
    "# 7. Distribución de errores (inferior izquierda)",
    "ax7 = fig.add_subplot(gs[2, 0])",
    "errores_por_clase = []",
    "for i, clase in enumerate(le_target.classes_):",
    "    class_errors = ((y_test == i) & (best_predictions != i)).sum()",
    "    class_total = (y_test == i).sum()",
    "    if class_total > 0:",
    "        error_rate = class_errors / class_total",
    "        errores_por_clase.append(error_rate)",
    "    else:",
    "        errores_por_clase.append(0)",
    "ax7.bar(classes, errores_por_clase, alpha=0.8, color='#e74c3c')",
    "ax7.set_xlabel('Clases', fontweight='bold')",
    "ax7.set_ylabel('Tasa de Error', fontweight='bold')",
    "ax7.set_title('Distribución de Errores por Clase', fontsize=12, fontweight='bold', pad=20)",
    "ax7.grid(True, alpha=0.3, axis='y')",
    "for i, v in enumerate(errores_por_clase):",
    "    ax7.text(i, v + 0.01, f'{v*100:.1f}%', ha='center', va='bottom', fontweight='bold')",
    "",
    "# 8. Top 3 modelos (inferior centro)",
    "ax8 = fig.add_subplot(gs[2, 1])",
    "ax8.axis('off')",
    "top3_text = \"🏆 TOP 3 MODELOS\\n\" + \"=\"*40 + \"\\n\\n\"",
    "for i, (model_name, metrics) in enumerate(results_sorted_clf.head(3).iterrows(), 1):",
    "    medal = \"🥇\" if i == 1 else (\"🥈\" if i == 2 else \"🥉\")",
    "    top3_text += f\"{medal} {i}. {model_name}\\n\"",
    "    top3_text += f\"   Accuracy: {metrics['Accuracy_test']:.4f}\\n\"",
    "    top3_text += f\"   F1-Score: {metrics['F1_test']:.4f}\\n\"",
    "    top3_text += f\"   Precision: {metrics['Precision_test']:.4f}\\n\"",
    "    top3_text += f\"   Recall: {metrics['Recall_test']:.4f}\\n\\n\"",
    "ax8.text(0.1, 0.9, top3_text, transform=ax8.transAxes, fontsize=10,",
    "         verticalalignment='top', family='monospace',",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))",
    "ax8.set_title('Top 3 Modelos', fontsize=12, fontweight='bold', pad=20)",
    "",
    "# 9. Resumen estadístico (inferior derecha)",
    "ax9 = fig.add_subplot(gs[2, 2])",
    "ax9.axis('off')",
    "total_samples = len(y_test)",
    "correct_predictions = (y_test == best_predictions).sum()",
    "incorrect_predictions = total_samples - correct_predictions",
    "accuracy = correct_predictions / total_samples",
    "",
    "resumen_text = \"📊 RESUMEN ESTADÍSTICO\\n\" + \"=\"*40 + \"\\n\\n\"",
    "resumen_text += f\"Total de muestras: {total_samples}\\n\"",
    "resumen_text += f\"Predicciones correctas: {correct_predictions}\\n\"",
    "resumen_text += f\"Predicciones incorrectas: {incorrect_predictions}\\n\"",
    "resumen_text += f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\\n\\n\"",
    "resumen_text += f\"Distribución de clases:\\n\"",
    "for i, clase in enumerate(le_target.classes_):",
    "    count = (y_test == i).sum()",
    "    percentage = (count / total_samples) * 100",
    "    resumen_text += f\"  • {clase}: {count} ({percentage:.1f}%)\\n\"",
    "",
    "ax9.text(0.1, 0.9, resumen_text, transform=ax9.transAxes, fontsize=10,",
    "         verticalalignment='top', family='monospace',",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))",
    "ax9.set_title('Estadísticas', fontsize=12, fontweight='bold', pad=20)",
    "",
    "# Título general del dashboard",
    "fig.suptitle(f'DASHBOARD DE CLASIFICACIÓN DE BIOMASA - Análisis Completo ({datetime.now().strftime(\"%Y-%m-%d %H:%M\")})', ",
    "             fontsize=16, fontweight='bold', y=0.98)",
    "",
    "plt.show()",
    "",
    "print(\"✓ Dashboard generado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Función de Predicción de Nuevos Datos desde Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_desde_excel_clasificacion(archivo_excel, nombre_hoja='Datos Nuevos'):\n    \"\"\"\n    Carga datos nuevos desde un archivo Excel y realiza predicciones de clasificación\n    \n    Parámetros:\n    -----------\n    archivo_excel : str\n        Ruta al archivo Excel con datos nuevos\n    nombre_hoja : str\n        Nombre de la hoja con los datos (por defecto 'Datos Nuevos')\n    \n    El archivo Excel debe contener las mismas columnas que se usaron para entrenar el modelo\n    \n    Retorna:\n    --------\n    DataFrame con predicciones y archivo Excel con resultados\n    \"\"\"\n    \n    try:\n        # Cargar datos nuevos\n        print(f\"Cargando datos desde: {archivo_excel}\")\n        df_nuevo = pd.read_excel(archivo_excel, sheet_name=nombre_hoja)\n        print(f\"✓ Datos cargados: {df_nuevo.shape[0]} registros\")\n        \n        # Verificar columnas requeridas\n        columnas_requeridas = X_imputed.columns.tolist()\n        columnas_faltantes = [col for col in columnas_requeridas if col not in df_nuevo.columns]\n        \n        if columnas_faltantes:\n            print(f\"❌ Error: Faltan las siguientes columnas:\")\n            for col in columnas_faltantes:\n                print(f\"   - {col}\")\n            return None\n        \n        # Preparar datos (mismas transformaciones que en entrenamiento)\n        X_nuevo = df_nuevo[columnas_requeridas].copy()\n        \n        # Codificar variables categóricas si es necesario\n        categorical_features = X_nuevo.select_dtypes(include=['object']).columns\n        le_features_nuevo = LabelEncoder()\n        for col in categorical_features:\n            X_nuevo[col] = le_features_nuevo.fit_transform(X_nuevo[col].astype(str))\n        \n        # Imputar valores nulos con la misma estrategia\n        imputer_nuevo = SimpleImputer(strategy='median')\n        X_nuevo_imputed = pd.DataFrame(imputer_nuevo.fit_transform(X_nuevo), columns=X_nuevo.columns)\n        \n        # Escalar datos si el modelo lo requiere\n        usa_escalado = best_model_name in ['SVM Linear', 'SVM RBF', 'SVM Polynomial', 'Logistic Regression', 'KNN', 'Naive Bayes']\n        \n        if usa_escalado:\n            scaler_nuevo = StandardScaler()\n            X_nuevo_scaled = scaler_nuevo.fit_transform(X_nuevo_imputed)\n            X_procesado = X_nuevo_scaled\n        else:\n            X_procesado = X_nuevo_imputed\n        \n        # Realizar predicciones\n        print(f\"",
    "Realizando predicciones con {best_model_name}...\")\n        \n        if hasattr(best_model, 'best_estimator_'):\n            predicciones = best_model.best_estimator_.predict(X_procesado)\n            if hasattr(best_model.best_estimator_, 'predict_proba'):\n                probabilidades = best_model.best_estimator_.predict_proba(X_procesado)\n        else:\n            predicciones = best_model.predict(X_procesado)\n            if hasattr(best_model, 'predict_proba'):\n                probabilidades = best_model.predict_proba(X_procesado)\n        \n        # Decodificar predicciones\n        clases_predichas = [le_target.classes_[i] for i in predicciones]\n        \n        # Crear DataFrame de resultados\n        df_resultados = df_nuevo.copy()\n        df_resultados['Clase_Predicha'] = clases_predichas\n        df_resultados['Clase_Predicha_Codigo'] = predicciones\n        \n        # Agregar probabilidades si están disponibles\n        if 'probabilidades' in locals():\n            for i, clase in enumerate(le_target.classes_):\n                df_resultados[f'Prob_{clase}'] = probabilidades[:, i]\n        \n        df_resultados['Modelo_Utilizado'] = best_model_name\n        df_resultados['Fecha_Prediccion'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        \n        # Guardar resultados en Excel\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        archivo_salida = f'Predicciones_Clasificacion_{timestamp}.xlsx'\n        \n        with pd.ExcelWriter(archivo_salida, engine='openpyxl') as writer:\n            # Hoja 1: Predicciones completas\n            df_resultados.to_excel(writer, sheet_name='Predicciones', index=False)\n            \n            # Hoja 2: Resumen estadístico\n            resumen = pd.DataFrame({\n                'Métrica': [\n                    'Total de registros',\n                    'Modelo utilizado',\n                    'Fecha de predicción',\n                    'Clases posibles',\n                    '',\n                    'Distribución de predicciones:',\n                ],\n                'Valor': [\n                    len(df_resultados),\n                    best_model_name,\n                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                    ', '.join(le_target.classes_),\n                    '',\n                    '',\n                ]\n            })\n            \n            # Agregar distribución de clases predichas\n            for clase in le_target.classes_:\n                count = (df_resultados['Clase_Predicha'] == clase).sum()\n                percentage = (count / len(df_resultados)) * 100\n                resumen = pd.concat([resumen, pd.DataFrame({\n                    'Métrica': [f'  {clase}'],\n                    'Valor': [f'{count} ({percentage:.1f}%)']\n                })], ignore_index=True)\n            \n            resumen.to_excel(writer, sheet_name='Resumen', index=False)\n            \n            # Formatear hojas\n            workbook = writer.book\n            for sheet_name in workbook.sheetnames:\n                worksheet = workbook[sheet_name]\n                for column in worksheet.columns:\n                    max_length = 0\n                    column_letter = column[0].column_letter\n                    for cell in column:\n                        try:\n                            if len(str(cell.value)) > max_length:\n                                max_length = len(cell.value)\n                        except:\n                            pass\n                    adjusted_width = min(max_length + 2, 50)\n                    worksheet.column_dimensions[column_letter].width = adjusted_width\n        \n        print(f\"",
    "✓ Predicciones completadas exitosamente\")\n        print(f\"✓ Resultados guardados en: {archivo_salida}\")\n        print(f\"",
    "Distribución de predicciones:\")\n        for clase in le_target.classes_:\n            count = (df_resultados['Clase_Predicha'] == clase).sum()\n            percentage = (count / len(df_resultados)) * 100\n            print(f\"  {clase}: {count} ({percentage:.1f}%)\")\n        \n        return df_resultados, archivo_salida\n        \n    except FileNotFoundError:\n        print(f\"❌ Error: No se encontró el archivo {archivo_excel}\")\n        return None\n    except Exception as e:\n        print(f\"❌ Error al procesar datos: {str(e)}\")\n        return None\n\n# Función para crear archivo de ejemplo\ndef crear_excel_ejemplo_clasificacion():\n    \"\"\"Crea un archivo Excel de ejemplo para predicciones de clasificación\"\"\"\n    \n    # Generar datos de ejemplo\n    np.random.seed(42)\n    n_ejemplos = 10\n    \n    # Tomar algunos datos del conjunto de prueba como ejemplo\n    indices_ejemplo = np.random.choice(X_test.index, size=min(n_ejemplos, len(X_test)), replace=False)\n    datos_ejemplo = X_imputed.loc[indices_ejemplo].copy()\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f'Datos_Nuevos_Ejemplo_Clasificacion_{timestamp}.xlsx'\n    datos_ejemplo.to_excel(filename, sheet_name='Datos Nuevos', index=False)\n    \n    print(f\"✓ Archivo de ejemplo creado: {filename}\")\n    print(f\"  Contiene {len(datos_ejemplo)} registros de ejemplo\")\n    print(f\"",
    "Para realizar predicciones, use:\")\n    print(f\"  resultados, archivo = predecir_desde_excel_clasificacion('{filename}')\")\n    \n    return filename\n\n# Crear archivo de ejemplo\nif 'X_imputed' in globals() and 'best_model' in globals():\n    archivo_ejemplo = crear_excel_ejemplo_clasificacion()\nelse:\n    print('⚠ Ejecute primero las celdas anteriores para entrenar el modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Guardado del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\nimport joblib\nimport json\n\n# Guardar el mejor modelo y objetos necesarios\ndef guardar_modelo_clasificacion():\n    \"\"\"\n    Guarda el modelo entrenado, scaler, imputer y metadatos\n    para uso posterior en producción\n    \"\"\"\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    print(\"=\" * 70)\n    print(\"GUARDANDO MODELO PARA SISTEMA DE PREDICCIÓN EN EXCEL\")\n    print(\"=\" * 70)\n    \n    # 1. Guardar el mejor modelo\n    if hasattr(best_model, 'best_estimator_'):\n        modelo_final = best_model.best_estimator_\n    else:\n        modelo_final = best_model\n    \n    joblib.dump(modelo_final, 'best_model_clasificacion.pkl')\n    print(f\"✓ Modelo guardado: best_model_clasificacion.pkl\")\n    \n    # 2. Guardar el scaler (si se usó)\n    usa_escalado = best_model_name in ['SVM Linear', 'SVM RBF', 'SVM Polynomial', 'Logistic Regression', 'KNN', 'Naive Bayes']\n    \n    if usa_escalado:\n        scaler_guardado = StandardScaler()\n        scaler_guardado.fit(X_train)\n        joblib.dump(scaler_guardado, 'scaler_clasificacion.pkl')\n        print(f\"✓ Scaler guardado: scaler_clasificacion.pkl\")\n    \n    # 3. Guardar el imputer\n    imputer_guardado = SimpleImputer(strategy='median')\n    imputer_guardado.fit(X_train)\n    joblib.dump(imputer_guardado, 'imputer_clasificacion.pkl')\n    print(f\"✓ Imputer guardado: imputer_clasificacion.pkl\")\n    \n    # 4. Guardar el LabelEncoder\n    joblib.dump(le_target, 'label_encoder.pkl')\n    print(f\"✓ LabelEncoder guardado: label_encoder.pkl\")\n    \n    # 5. Guardar metadatos\n    metadata = {\n        'modelo': best_model_name,\n        'fecha_entrenamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'metricas': {\n            'accuracy_test': float(results_sorted_clf.loc[best_model_name, 'Accuracy_test']),\n            'precision_test': float(results_sorted_clf.loc[best_model_name, 'Precision_test']),\n            'recall_test': float(results_sorted_clf.loc[best_model_name, 'Recall_test']),\n            'f1_test': float(results_sorted_clf.loc[best_model_name, 'F1_test']),\n            'cv_accuracy_mean': float(results_sorted_clf.loc[best_model_name, 'CV_Accuracy_mean']),\n            'cv_accuracy_std': float(results_sorted_clf.loc[best_model_name, 'CV_Accuracy_std'])\n        },\n        'clases': le_target.classes_.tolist(),\n        'variables_predictoras': X_imputed.columns.tolist(),\n        'usa_escalado': usa_escalado,\n        'parametros': best_model.best_params_ if hasattr(best_model, 'best_params_') else {}\n    }\n    \n    with open('model_info_clasificacion.json', 'w') as f:\n        json.dump(metadata, f, indent=2)\n    print(f\"✓ Metadatos guardados: model_info_clasificacion.json\")\n    \n    # 6. Crear backup del modelo completo\n    backup_data = {\n        'modelo': modelo_final,\n        'scaler': scaler_guardado if usa_escalado else None,\n        'imputer': imputer_guardado,\n        'label_encoder': le_target,\n        'metadata': metadata\n    }\n    \n    backup_filename = f'modelo_clasificacion_backup_{timestamp}.pkl'\n    joblib.dump(backup_data, backup_filename)\n    print(f\"✓ Backup completo guardado: {backup_filename}\")\n    \n    print(\"",
    "\" + \"=\" * 70)\n    print(\"RESUMEN DE ARCHIVOS GUARDADOS:\")\n    print(\"=\" * 70)\n    archivos = [\n        (\"best_model_clasificacion.pkl\", \"Modelo entrenado\"),\n        (\"scaler_clasificacion.pkl\", \"Escalador (si aplica)\" if usa_escalado else \"No aplica\"),\n        (\"imputer_clasificacion.pkl\", \"Imputador de valores nulos\"),\n        (\"label_encoder.pkl\", \"Codificador de clases\"),\n        (\"model_info_clasificacion.json\", \"Metadatos y métricas\"),\n        (backup_filename, \"Backup completo\"),\n    ]\n    \n    for archivo, descripcion in archivos:\n        if descripcion != \"No aplica\":\n            print(f\"  • {archivo:<35} - {descripcion}\")\n    \n    print(\"",
    "\" + \"=\" * 70)\n    print(\"PRÓXIMOS PASOS:\")\n    print(\"=\" * 70)\n    print(\"1. Ejecuta: python 2_crear_plantilla_excel_clasificacion.py\")\n    print(\"2. Llena los datos en el Excel generado\")\n    print(\"3. Ejecuta: python 3_predecir_en_excel_clasificacion.py\")\n    print(\"4. Abre el Excel para ver las predicciones\")\n    print(\"=\" * 70)\n    \n    return 'best_model_clasificacion.pkl', 'scaler_clasificacion.pkl', 'model_info_clasificacion.json', backup_filename\n\n\n# Guardar el modelo\nif 'best_model' in globals() and 'best_model_name' in globals():\n    archivos_guardados = guardar_modelo_clasificacion()\nelse:\n    print('⚠ Ejecute primero las celdas anteriores para entrenar el modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Resumen Final y Generación de Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen completo de archivos\ndef generar_resumen_archivos():\n    \"\"\"Genera un archivo de texto con resumen de todos los archivos generados\"\"\"\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename_resumen = f'RESUMEN_ARCHIVOS_{timestamp}.txt'\n    \n    archivos_info = []\n    \n    # 1. Modelo\n    archivos_info.append({\n        'categoria': '🤖 MODELO DE CLASIFICACIÓN',\n        'archivos': [\n            {\n                'nombre': 'best_model_clasificacion.pkl',\n                'descripcion': 'Modelo de clasificación entrenado',\n                'tamaño': 'Variable según modelo'\n            },\n            {\n                'nombre': 'model_info_clasificacion.json',\n                'descripcion': 'Metadatos y métricas del modelo',\n                'tamaño': 'Pocos KB'\n            }\n        ]\n    })\n    \n    # 2. Reporte Excel\n    if 'archivo_reporte' in globals():\n        archivos_info.append({\n            'categoria': '📊 REPORTE DE ANÁLISIS',\n            'archivos': [\n                {\n                    'nombre': archivo_reporte,\n                    'descripcion': 'Reporte completo con resultados (4 hojas)',\n                    'tamaño': 'Variable'\n                }\n            ]\n        })\n    \n    # 3. Archivos de ejemplo\n    if 'archivo_ejemplo' in globals():\n        archivos_info.append({\n            'categoria': '📁 DATOS DE EJEMPLO',\n            'archivos': [\n                {\n                    'nombre': archivo_ejemplo,\n                    'descripcion': 'Plantilla con datos de ejemplo para predicciones',\n                    'tamaño': 'Pocos KB'\n                }\n            ]\n        })\n    \n    # Escribir resumen\n    with open(filename_resumen, 'w', encoding='utf-8') as f:\n        f.write(\"=\" * 80 + \"",
    "\")\n        f.write(\"RESUMEN DE ARCHIVOS GENERADOS - CLASIFICACIÓN DE BIOMASA",
    "\")\n        f.write(\"=\" * 80 + \"",
    "",
    "\")\n        f.write(f\"Fecha de generación: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
    "\")\n        f.write(f\"Mejor modelo: {best_model_name}",
    "\")\n        f.write(f\"Accuracy: {results_sorted_clf.loc[best_model_name, 'Accuracy_test']:.4f}",
    "\")\n        f.write(f\"F1-Score: {results_sorted_clf.loc[best_model_name, 'F1_test']:.4f}",
    "",
    "\")\n        \n        for categoria_info in archivos_info:\n            f.write(categoria_info['categoria'] + \"",
    "\")\n            f.write(\"-\" * 80 + \"",
    "\")\n            for archivo in categoria_info['archivos']:\n                f.write(f\"  • {archivo['nombre']}",
    "\")\n                f.write(f\"    {archivo['descripcion']}",
    "",
    "\")\n        \n        f.write(\"=\" * 80 + \"",
    "\")\n        f.write(\"CONTENIDO DE LOS ARCHIVOS:",
    "\")\n        f.write(\"=\" * 80 + \"",
    "",
    "\")\n        \n        resumen_contenido = [\n            [\"Modelo (.pkl/.joblib)\", \"Modelo entrenado + Scaler + Imputer + LabelEncoder\"],\n            [\"Metadatos (.json)\", \"Información del modelo, métricas y parámetros\"],\n            [\"Reporte Excel\", \"4 hojas: Resumen, Comparación, Predicciones, Variables\"],\n            [\"Datos Ejemplo\", \"Plantilla para nuevas predicciones\"],\n        ]\n        \n        max_width = max(len(row[0]) for row in resumen_contenido)\n        for fila in resumen_contenido:\n            f.write(f\"  {fila[0]:<{max_width+5}} | {fila[1]}",
    "\")\n        \n        f.write(\"",
    "\" + \"=\" * 80 + \"",
    "\")\n        f.write(\"💡 INSTRUCCIONES DE USO:",
    "\")\n        f.write(\"-\" * 80 + \"",
    "\")\n        f.write(\"\"\"\n  1. PARA REALIZAR NUEVAS PREDICCIONES:\n     • Prepare un archivo Excel con las columnas requeridas\n     • Use: predecir_desde_excel_clasificacion('su_archivo.xlsx')\n     \n  2. PARA CARGAR EL MODELO EN OTRA SESIÓN:\n     • import joblib\n     • modelo = joblib.load('best_model_clasificacion.pkl')\n     • predicciones = modelo.predict(datos_nuevos)\n     \n  3. PARA REVISAR RESULTADOS:\n     • Abra el archivo de reporte Excel\n     • Revise cada hoja para análisis detallado\n     \n  4. PARA COMPARTIR EL MODELO:\n     • Comparta los archivos .pkl y .json\n     • Incluya la lista de variables predictoras\n\"\"\")\n        f.write(\"=\" * 80 + \"",
    "\")\n    \n    print(f\"✓ Resumen de archivos generado: {filename_resumen}\")\n    return filename_resumen\n\n# Generar resumen\nif 'best_model_name' in globals():\n    archivo_resumen = generar_resumen_archivos()\n    \n    # Realizar predicciones de prueba\n    if 'archivo_ejemplo' in globals():\n        print(\"",
    "🔮 REALIZANDO PREDICCIONES DE PRUEBA...\")\n        print(\"-\" * 70)\n        resultados_prediccion, archivo_predicciones = predecir_desde_excel_clasificacion(archivo_ejemplo)\nelse:\n    print('⚠ Ejecute primero las celdas anteriores para entrenar el modelo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}